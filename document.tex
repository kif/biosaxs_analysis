%------------------------------------------------------------------------------
% Template file for the submission of papers to IUCr journals in LaTeX2e
% using the iucr document class
% Copyright 1999-2013 International Union of Crystallography
% Version 1.6 (28 March 2013)
%------------------------------------------------------------------------------

\documentclass[preprint]{iucr}              % DO NOT DELETE THIS LINE

     %-------------------------------------------------------------------------
     % Information about journal to which submitted
     %-------------------------------------------------------------------------
     \journalcode{S}              % Indicate the journal to which submitted
                                  %   A - Acta Crystallographica Section A
                                  %   B - Acta Crystallographica Section B
                                  %   C - Acta Crystallographica Section C
                                  %   D - Acta Crystallographica Section D
                                  %   E - Acta Crystallographica Section E
                                  %   F - Acta Crystallographica Section F
                                  %   J - Journal of Applied Crystallography
                                  %   M - IUCrJ
                                  %   S - Journal of Synchrotron Radiation
\usepackage{graphicx}
%\graphicspath{ {./} }https://www.overleaf.com/project/624fd103d67e172ab9fc6700

\begin{document}                  % DO NOT DELETE THIS LINE

     %-------------------------------------------------------------------------
     % The introductory (header) part of the paper
     %-------------------------------------------------------------------------

     % The title of the paper. Use \shorttitle to indicate an abbreviated title
     % for use in running heads (you will need to uncomment it).

\title{New data analysis for BioSAXS at the ESRF}
%\shorttitle{Short Title}

     % Authors' names and addresses. Use \cauthor for the main (contact) author.
     % Use \author for all other authors. Use \aff for authors' affiliations.
     % Use lower-case letters in square brackets to link authors to their
     % affiliations; if there is only one affiliation address, remove the [a].

\cauthor[a]{Jérôme}{Kieffer}{jerome.kieffer@esrf.fr}{}
\author[b]{Martha}{Brennich}
\author[b]{Jean-Baptiste}{Florial}
\author[a]{Marcus}{Oscarsson}
\author[a]{Alejandro}{De Maria Antolinos}
\author[a]{Mark}{Tully}
\author[a]{Petra}{Pernot}
\aff[a]{The European Synchrotron, 71 Avenue des Martyrs, 38000 Grenoble \country{France}}
\aff[b]{European Molecular Biology Laboratory, 71 Avenue des Martyrs, 38000 Grenoble \country{France}}


     % Use \shortauthor to indicate an abbreviated author list for use in
     % running heads (you will need to uncomment it).

\shortauthor{Kieffer and Brennich}

     % Use \vita if required to give biographical details (for authors of
     % invited review papers only). Uncomment it.

%\vita{Author's biography}

     % Keywords (required for Journal of Synchrotron Radiation only)
     % Use the \keyword macro for each word or phrase, e.g. 
     % \keyword{X-ray diffraction}\keyword{muscle}

\keyword{BioSAXS; online data analysis; solution scattering; proteins; biological small-angle X-ray scattering; automation; high brilliance; structural biology;  high-throughput SAXS;  size-exclusion chromatography; online purification}

     % PDB and NDB reference codes for structures referenced in the article and
     % deposited with the Protein Data Bank and Nucleic Acids Database (Acta
     % Crystallographica Section D). Repeat for each separate structure e.g
     % \PDBref[dethiobiotin synthetase]{1byi} \NDBref[d(G$_4$CGC$_4$)]{ad0002}

%\PDBref[optional name]{refcode}
%\NDBref[optional name]{refcode}

\maketitle                        % DO NOT DELETE THIS LINE

\begin{synopsis}
Detailed presentation of the automatic data analysis pipelines for the BioSAXS beam-line at the European synchrotron.  
\end{synopsis}

\begin{abstract}
The second phase of the ESRF upgrade program did not only provide a new storage ring (Extremely Brilliant Source, EBS), it also allowed to refurbish several beam-lines.
The BioSAXS beamline (located on port BM29) was upgraded with a new wiggler source and a larger detector. 
All analysis software has been rewritten to cope with the increased data-flux and continue providing beam-line users with reduced and pre-processed data in real-time.
This article describes \textit{FreeSAS}, an open-source collection of various SAS analysis algorithms needed to reduce and analyze BioSAXS data, and \textit{dahu}, the tool used to interface data-analysis with beam-line control. 
It further presents the data processing pipelines for the different data acquisitions modes of the beam-line, using either a sample-changer for individual homogeneous samples or an inline size-exclusion chromatography setup.
\end{abstract}


     %-------------------------------------------------------------------------
     % The main body of the paper
     %-------------------------------------------------------------------------
     % Now enter the text of the document in multiple \section's, \subsection's
     % and \subsubsection's as required.

\section{Introduction}
Small angle scattering (SAS) provides information on the shape of macro-molecules on the nanometer scale and is particularly suited for biological samples thanks to a large range of suitable buffer conditions.
Unlike single crystal diffraction or Nuclear Magnetic Resonance (NMR) which offer atomic resolution, BioSAS provides only information on the envelope of macro-molecules: it allows to validate the relative position of large structures in the assembly of biological complexes \cite{biosaxs_rev2018}. 
Structural biologists perform \textsc{saxs} experiments to validate the size and the shape of their protein or complex under study. 
Since most beam-line users are biologists, they are neither synchrotron, nor \textsc{saxs} specialists.
A full analysis of their data is thus of crucial importance to them \cite{shibom}. 


Whereas most BioSAXS beam-lines throughout the world have focused on the graphical user interface for helping their users with data analysis \cite{dawn, als-biosaxs}, the BM29 beam-line from the European synchrotron \cite{BM29paper} has focused on fully automated data analysis which provides user not only with the reduced curves and pre-analyzed data, but also with all metadata and parameters needed to reprocess their data and get them published according to the relevant guidelines \cite{guidelines_2017}.
%These metadata are also important to reprocess data using third party software to ensure comparable results. 
BM29 had an automated pipeline for the data-analysis which was based on \textsc{edna} \cite{edna} and was using the \textsc{atsas2} \cite{ATSAS2} software underneath. 
While the outcome of these processing was very appreciated by beam-line users, the system was already close to the maximal throughput possible in terms of performances. 
The new EBS source (Extremely Brilliant Source, \citeasnoun{EBS} of the ESRF not only provides a higher brilliance, but also new wiggler sources for the former bending-magnet beam-lines which triggered the re-build and the upgrade of most of them. 
The BM29 BioSAXS beam-line has been rebuilt in 2019 and now features a 2-pole wiggler source and a new Pilatus3 2M detector designed to be mounted in vacuum (previously 1M). 
This has lead to a substantial increase of data-rate; mainly due to the larger detector. 

This paper is divided into two main parts and starts with a presentation of the tools developed for online data analysis of \textsc{saxs} data \textit{FreeSAS} and \textit{dahu}, the job scheduler.
The section 3 presents the three different pipelines used at the beam-line: the common pipeline for the reduction of scattering images, the pipeline used with the sample-changer and the one used with the size exclusion chromatography setup.

\section{Tools}
During the upgrade of the beam-line, most software have been replaced by completely new developments: the sequencer \texttt{spec} \cite{spec} was replaced by \textsc{bliss} \cite{bliss}, the graphical user interface BsxCuBE (Biosaxs Customized Beam-line Environment) was re-implemented with web-technology, replacing the PyQt4 interface, ...
To meet the real-time feed-back requirement, the software doing the analysis was also re-written.

Precise benchmark of the execution times (see also section \ref{performances}) of the previous \textsc{edna}-based pipeline \cite{BM29ODA} demonstrated that time was mostly spent in launching external tools coming from the \textsc{atsas2} suite and in parsing output files produced by those tools; not in the execution of those external programs.
It was decided to rewrite all pipelines in plain Python \cite{python} and to regroup all SAS-related analysis within a library, \textit{FreeSAS}, which would be easy to distribute. 
Pipelines \cite{dahu-bm29} are heavily optimized for running on the beam-line hardware thus cannot easily be reused despite their MIT-license.
Finally, this code is interfaced to the control software, \textsc{bliss} \cite{bliss}, via Tango \cite{tango} and uses a simple task scheduler, \textit{dahu}, already used at the \textsc{trusaxs} beam-line \cite{id02_2022}.

\subsection{Small angle scattering analysis tools, \textit{FreeSAS}}

\textit{FreeSAS} is a Python library \cite{python} containing SAS-analysis tools available both via a Python API and from the command line interface (program names are referenced to using a \texttt{monospaced} font). 
It does not claim to be as complete as the \textsc{atsas} counterpart \cite{ATSAS3},
but is free, released under the liberal MIT license (i.e. it can be included into commercial products), all source code is available publicly on github \cite{freesas} and it is open to external contributions.
Table \ref{freesas-atsas} summarizes this comparison.
Despite Python being an interpreted language, \textit{FreeSAS} is performance oriented and most of the processing is performed via Cython \cite{cython} extensions, written in C and compiled, to obtain the required performances. 
\textit{FreeSAS} has been made available and packaged independently from \textit{dahu} (the online analysis tools) and from the processing pipelines so that scientists can reprocess their data and compare their results with those of other analysis software like Sc\AA tter \cite{scatter}. 
The current release, \textit{FreeSAS} 0.9, supports Python 3.6 to 3.9.

\subsubsection{SAS plotting:} The \texttt{freesas} command line tool (provided by the \textit{FreeSAS} suite), figure \ref{plot}, provides a way to plot a semi-logarithmic representation of the SAS curve: $I=f(q)$, where $q = 4\pi sin(\theta)/\lambda$ is the amplitude of the scattering vector, $\theta$ is half the scattering angles, $\lambda$ the wavelength and $I$ the recorded intensity at a given $q$; along side with some basic analyses which are described in next sections.
The program takes as entry a 3-column \textsc{ascii} file containing $q$, $I$ and $\sigma$. 
% The unit of $q$ can be precised as command line argument.

\begin{figure}
\label{plot}
\includegraphics[width=12cm]{Figure_1.eps}
\caption{Default visualization of BioSAS data using the \texttt{freesas} tool on a 3-column \textsc{ascii}-file with q, I and $\sigma$, here BSA acquired with 12.5 keV photons. 
The top-left plot presents the classical $I=f(q)$ plot in semi-logarithmic scale with the Guinier region highlighted in green and IFT-based curve superimposed in red. Those results come from the Guinier analysis (top-right) based on a linear fit of $log(I)=f(q^2)$ and from the pair distribution function ($p=f(r)$) obtained from the BIFT analysis (bottom-right).
A Dimensionless Kratky plot is presented at the lower-left. 
}
\end{figure}

\subsubsection{Guinier-region fitting:}
The first analysis performed on BioSAS data is to determines the radius of gyration ($R_g$) of the solvated macro-molecule and the forwards scattering intensity, $I_0$ \cite{Guinier1955}.
Based on the Tailor expansion of the scattering curve at $q=0$, $R_g$ is obtained from the the linear regression of $log(I(q)) = log(I_0)-1/3 (R_{G}q)^{2}$ on the proper q-range, at small angles, called the Guinier region.
The selection of the Guinier-region is far from being obvious due to the beam-stop shadow, aggregation effects, and its upper limit depends on $R_g$ itself: $q \cdot R_g<1.3$.
Thus multiple implementation are provided: \texttt{free\_rg}, which derives from the implementation in BioXTAS-RAW \cite{bioxtasraw}; \texttt{free\_guinier}, which searches for a consensus region rather than the best possible Guinier-region; and finally \texttt{free\_gpa}, which performs a Guinier-peak-analysis \cite{gpa}. 
The later is a quick assessment of the $R_g$ and $I_0$, sometimes less robust than the two other implementations, but suitable when many data sets are to be analyzed like in chromatography mode.
It is worth mentioning that none of the three algorithms provide the exact same results as the original \textsc{autorg} \cite{ATSAS2} version from \textsc{atsas}. 
This highlights the importance of publishing the actual implementation of the algorithms with the associated numerical parameters used in it.
  
\subsubsection{Pair distribution function:}
Although the scattering curve $I(q)$ is the Fourier transform of the pair distribution function $p(r)$, the later cannot directly be obtained from an inverse Fourier transform (IFT) due to the loss of phase information and the limited amount of information in the scattering curve. 
This ill-posed mathematical problem has no exact solution and is usually inverted using some extra constraints, like the finite size of the macro-molecule (defined by its maximum diameter, $Dmax$).    
FreeSAS proposes an IFT based on the Bayesian statistics and derived from \textsc{bift} \cite{bift} and the implementation in BioXTAS-RAW \cite{BioXTAS}.
The implementation in \textit{FreeSAS}, called \texttt{free\_bift}, is based on Cython \cite{cython} and uses \textssc{blas} \cite{blas} and multi-threading for performances.
%The command-line program to perfrom and IFT is \textit{bift.py}. 
Despite their different approaches, \texttt{free\_bift} provides similar results to those of \textsc{datgnom} \cite{ATSAS1} from \textsc{atsas} which uses a Tikhonov's regularisation.

\subsubsection{Other tools available in \textit{FreeSAS}:}
\begin{itemize}
    \item \texttt{cormapy} evaluates radiation damage by comparing couples of frames using the correlation-map (CorMap) algorithm implemented from the publication by \citeasnoun{CorMap}.
%To obtain the best possible signal from the sample, the capillary containing flowing solution is exposed multiple times with a short exposure.
%All equivalent frames are then merged to optimized the signal/noise ratio without pollution from radiation damaged data.  
%The equivalence of a couple of frames is obtained from the correlation-map (CorMap) algorithm implemented from the publication by \citeasnoun{CorMap}. 
    \item \texttt{subpycomp} rotates and flips  bead models to overlay them prior merging them \cite{BM29ODA}.
    \item \texttt{extract\_ascii}: generate a text files in 3-column $q$, $I_{avg}$, $\sigma(I)$ format (with headers) for compatibility with third-party tools. Following ESRF's data-policy \cite{data-policy}, the default file-format used at the BioSAXS beam-line has changed to the hierarchical-data format, HDF5 \cite{hdf5}, for acquisition, analysis and archival of data. This is part of the \textsc{fair} principle \cite{FAIR} where the data-portal \cite{data-portal} provides  \textbf{F}indability, HDF5 provides \textbf{A}ccessibility, \texttt{extract\_ascii} provides  \textbf{I}nteroperability, hoping this data gets \textbf{R}eused after the embargo period.
\end{itemize}

The table \ref{freesas-atsas} provides some equivalence for program names provided by \textit{FreeSAS} and \textsc{atsas} packages.
The later toolbox is built on top of several decades of experience acquired the group of EMBL-Hamburg and features some sixty programs to perform BioSAS data analysis.
\textit{FreeSAS} cannot compete on the number of features but tries to offer a clean Python interface to build pipelines in an efficient manner and fully open (both for usage and modification).

\begin{table}
    \label{freesas-atsas}
    \caption{\textsc{Atsas} counter-parts to command line programs provided by \textit{FreeSAS}}
    \begin{center}
    \begin{tabular}{|c|c|}
        \hline
        FreeSAS & \textsc{atsas} \\
        \hline
        \texttt{cormapy} & \textsc{datcmp}\\
        \texttt{free\_gpa} / \texttt{free\_guinier} / \texttt{free\_rg}& \textsc{autorg} / \textsc{datrg}\\
        \texttt{free\_bift}& \textsc{gnom} / \textsc{datgnom} / \textsc{autognom} / \textsc{datft}\\
        \texttt{supycomb}& \textsc{supcomb} / \textsc{cifsup} \\
        \texttt{freesas}& \textsc{primus}\\
        \texttt{extract\_ascii}& - \\
        \hline
    \end{tabular}
    \end{center}
    
\end{table}

\subsection{The job-manager: \textit{dahu}}

The role of the job manager is to ensure all processing requested by the client (here BsxCuBE3, the graphical user interface) are actually performed, informs the client about the finished jobs and warns it in case of an error.

%(what do you mean by client? The requestor?) is informed in case of issues. %Only if issues arrise? Or is other information also returned?
%This subsection is purely technical and can be skipped. 

\textsc{Edna} was used as workflow manager for the previous data-analysis pipeline for many protein crystallography beam-lines \cite{edna} and for the BioSAXS beam-line at the ESRF \cite{BM29ODA}.
The parallelization model used in \textsc{edna} is based on Python threads and forking processes which was wasting resources in serializing and inter-process communication. 
%Despite the Global Interpreter Lock (GIL) which prevents Python from running multiple threads simultaneously, parallel processing occues when the work is performed in separated process. 
%This implies that for each task to be perfomed, an input file needs to be written before launching the process and the result file needs to be read and parsed after the end of the processing.
%When processing was doing loops of jobs, most of the time was spent in string manipulation for writing and parsing strings (for example: to compare pair-wise 10 frames it takes 45 process to be launched !)
 
%The \textit{dahu} job-manager was designed for the \textsc{trusaxs} beam-line \cite{id02_2022} with those limitations in mind and additional low latency constrain: some jobs, there, needs to be processed with a dozen of milliseconds. 
The \textit{dahu} job-manager was designed for the low latency constraints of the \textsc{trusaxs} beam-line \cite{id02_2022}, where some jobs need to be processed within a dozen of milliseconds.
Batch queuing systems, like \textsc{slurm} \cite{slurm}, are very efficient at distributing heavy jobs, but none was optimized for reduced scheduling time (or low latency).

%Text manipulation was simplified by switching from XML to JSON data-structure representation. 
%This modification alone has proven to speed up \textsc{edna} by a factor 3 !
The tango interface \cite{tango} implemented in \textit{dahu} was kept similar to the one in \textsc{edna} to ease the transition.
%, the adaptation was  framework were kept and the code was simplified to the extreme since \textit{dahu} represents only 1000 lines of code. 
The scheduling of jobs is performed via a shared queue and only few workers are running simultaneously in different threads.
Thus the code runs actually in parallel only in sections where the Global Interpreter Lock from Python (GIL) is released, like in Cython extensions \cite{cython} from FreeSAS or in the OpenCL code from pyFAI \cite{pyFAI_gpu}.



\subsubsection{Dahu-job}
manages the execution of one \textit{dahu-plugin} (see here-after) and provides an unique identifier which gives access to the status and output of the processing.
Jobs see their input and output saved onto disk, which allows off-line reprocessing in case of an issue during online data-analysis.

\subsubsection{Dahu-plugins} implement the processing logic of the different pipelines.
Written in simple Python and fairly independent from the \textit{dahu} framework, those plugins are often written or modified by beam-line scientists themselves.

\subsubsection{Offline re-processing}
is made possible by the \texttt{dahu-reprocess} command-line tool.
This tool was designed to (re-)execute one or several jobs based on the \textsc{json}-description file \cite{json} saved by the online data analysis server. 
Since \textit{dahu} has virtually no dependencies, it can be deployed on any computer to reprocess data. 
Nevertheless, to reprocess data acquired at the BioSAXS beam-line, one would need \textit{FreeSAS} and all the other dependencies of the BioSAXS \textit{plugins}, which are documented in the \textit{requirements.txt} file in the plugin directory available at GitHub \cite{dahu-bm29}.

\section{Data analysis pipelines}
\label{pipeline}
There are two main experiments performed at the BioSAXS beam-line, either using the sample-changer (referred to as SC) or the inline-chromatography setup (referred to as \textsc{hplc} since it uses a High Pressure Liquid Chromatograph).
Thus, two analysis pipelines were built, one for each of those experimental modes.
The common part, mainly dealing with azimuthal integration, is integrated into a pre-processing pipeline called \textit{integrate multi-frame}.
%This would profit from  description of how the data is actually structured in the two acquisition modes. I got completely lost here!

Since the Pilatus3 2M detector is controlled by the \textsc{lima} software \cite{lima}, raw images are now saved in a HDF5 file-format \cite{hdf5} with ten or hundred frames per file (depending on the acquisition mode, figure \ref{lima}).
HDF5 is not only imposed by the ESRF data policy  \cite{data-policy} but offers numerous benefits such as compression, faster data-access, symbolic links to data-sets stored in other files ...
% HDF5 offers compression, faster data-access, symbolic links to datasets from one file to another ... and is imposed by the ESRF data-policy \cite{data-policy}. 
This data-structure with several frames per file prevents us from re-using the former \textsc{edna} pipelines which were triggered frame by frame.

\begin{figure}
     \caption{Layout of a raw image HDF5-file produced by the \textsc{lima} acquisition software and visualized with the \textit{silx} viewer.}
     %\includegraphics[width=12cm]{lima.eps}
     \includegraphics[width=12cm]{Figure_2.eps}
     \label{lima}
\end{figure}

ESRF provides several tools to visualize those HDF5 files, most of them are either based on \textit{silx} \cite{silx} which is a graphical user interface based on Qt5 \cite{pyqt} or on the web-viewer \textit{h5web} \cite{h5web}, visualizing data inside a web-browser.
The \textit{h5web} library is already used in the beam-line control user interface BsxCuBE3 \cite{bm29_2022} and in the ESRF data-portal \cite{data-portal} where data are automatically catalogued and made accessible to the experimental team (figure \ref{dataportal}) during the embargo period, before making them publicly available, following the ESRF data-policy \cite{data-policy}. 

\begin{figure}
     \caption{Default visualization offered by the ESRF data-portal (https://data.esrf.fr) of a stack of frames acquired at the BioSAXS beam-line, using the \textit{h5web} viewer.}
     %\includegraphics[width=12cm]{dataportal.eps}
     \includegraphics[width=12cm]{Figure_3.eps}
     \label{dataportal}
\end{figure}

Since \textsc{lima} saves frames containing no metadata beside the camera configuration (figure \ref{lima}), all sample and experiment description (geometry, mask, \ldots), beam-stop diode intensities and other processing parameters have to be provided by the experiment sequencer, \textsc{bliss}, as part of the job description when triggering the process. 
This configuration can be retrieved from the data-portal web page (figure \ref{dataportal}) as a \textsc{json}-file \cite{json}, and is suitable to reprocess the raw-data.

The versatility of the HDF5 format allows to have one single output file for all results produced by a processing pipeline, making archival easier.
Each pipeline registers the result of every individual processing step of the pipeline in the output HDF5 file (as HDF5-groups), together with the configuration associated with each of the processing.
Input data sets are referenced using external links, which avoids data duplication while keeping traceability. 
Finally, metadata describing the sample, its buffer and the configuration of the beam-line are also recorded using the Nexus convention \cite{nexus}.
Each processing pipeline defines a default plot which tries to summarize the experimental result to the user.

\subsection{Multi-frame integration pipeline}
\label{multiframe_pipeline}
The multi-frame integration pipeline, figure \ref{multiframe_worflow}, is triggered with the name of one \textsc{lima}-file (containing several frames) and additional metadata describing the sample and the experiment.
Those additional information are either collected by the sequencer, \textsc{bliss}, like the beam-stop diode intensity, or read from the user interface, BSXCube3, like the sample name and concentration.

\begin{figure}
     \label{multiframe_worflow}
     \begin{center} 
     \caption{Schematic of the multi-frame integration pipeline: 
     1. azimuthal integration of individual frames; 
     2. comparison of 1d-curves (the 9 first frames are equivalent, the 10$^{th}$ is discarded);
     3. equivalent frames are averaged;
     4. azimuthal integration of the averaged image.}
     %\includegraphics[width=12cm]{multiframe_pipeline.eps}
     \includegraphics[width=12cm]{Figure_4.eps}
     \end{center}
\end{figure}

The figure \ref{multiframe} presents a file produced by this processing pipeline with the default plot consisting in the semi-logarithmic representation of the scattering curve $I(q)$ viewed with \textit{silx}.
This pipeline is built of four subsequent analysis steps, as illustrated in figure \ref{multiframe_worflow}:
\begin{enumerate}
\item Each recorded image is azimuthally integrated with \textit{pyFAI} \cite{pyfai_2020} to produce one scattering curve per frame;
\item Scattering curves are compared, searching for radiation damage using the \textit{CorMap} algorithm \cite{CorMap}; the probability for each pair of curves to be the same is compared to thresholds to assess their equivalence, those thresholds depend if frames were adjacent or not;
\item Equivalent images are averaged pixel-wise, weighted by the beam-stop diode intensity; variance is assessed assuming a Poisson statistics. Since time-averaging and azimuthal-integration are not commutative, (see appendix \ref{rational}), the processing restarts from 2D frames and not from individual curves;
\item The averaged frame is finally azimuthally integrated and uncertainties propagated accordingly. 
\end{enumerate}

\begin{figure}
     \caption{Layout of a HDF5-file obtained from the multi-frame integration pipeline, visualized with the \textit{silx} viewer.
     The HDF5-tree structure (left-hand side) follows the pipeline described in figure \ref{multiframe_worflow}.
     The right-hand side plot is the averaged scattering curve $I=f(q)$ in semi-logarithmic scale of the macro-molecule (BSA) before subtraction of the background signal.}
     %\includegraphics[width=12cm]{multiframe.eps}
     \includegraphics[width=12cm]{Figure_5.eps}
     \label{multiframe}
\end{figure}

The plot of the azimuthally integrated averaged frame (at stage 4) is set as the default display  when processing data in sample-changer mode.
In \textsc{hplc}-mode, the default plot represents the summed intensity as function of time, which is a fraction of the complete chromatogram. 
The HDF5-file additionally includes external links to the raw frames as acquired by the detector (stage 0).
%Uncertainties on pixel intensities are assumed to be Poissonian and propagated accordingly.
%Azimuthal integration is a common reduction step for all small angle scattering experiments. 
%The BioSAXS beamline uses \textit{pyFAI} \cite{pyfai_2020} as core of its \textit{multiframe} integration pipeline.
%Thanks to the GPU-computing, \textit{pyFAI} is able to integrate each frame within a millisecond.
%In order to avoid cross-correlation between neighboring bins, pixel splitting is disabled.
%Frames are read from the HDF5 file produced by \textit{LImA} \cite{lima} and all other metadata like the sample description and the transmitted  intensity (recorded on the beam-stop diode) are sent as part of the job description from BsxCuBE3, the graphical user interface running the beamline.

%In sample changer mode, all frames are expected to be similar and needs to be merged, except the ones exhibiting radiation damage which should be discarded.
%For this, every single scattering curve is compared with all others to build the correlation-map \cite{CorMap}. 
%This map allows to determine the largest group of frames which are equivalent based on two threshold, the minimum probability for two adjacent frames to be the same and the threshold for frames which are not adjacent. 

%Frames found equivalent in the correlation-map analysis are averaged taking into account normalization from the beam-stop diode value. 
%The uncertainties for pixel values is propagated assuming Poisson statistics. 
%The standard deviation of each pixel in the stack of equivalent frames was found noticably lower than the propagated variance, assuming a Poisson statistic.
%The averaged frame (with its associated uncertainties) is finally azimuthally integrated with pyFAI and stored in the output file. 
%The SAXS plot $I = f(q)$ is set as the default representation of this output file.

%In chromatography mode, the acquisition occurs typically over half an hour representing thousand images at 1Hz acquisition rate. 
%This results in several HDF5-files containing partial chromatograms.
%The total scattering is computed by summing all intensities in each of the curve to build a simple chromatogram $I_{sum} = f(t)$ 
%which indicates weather a sample eluted from the chromatography column or not. 
%This (partial) chromatogram is the default plot for this type of integrated file.



\subsection{Sample-changer pipeline}
\label{sc-pipeline}
In sample-changer mode, solutions containing samples are acquired alternatively with pure buffer solutions.
The throughput of the beam-line is then limited by the pipetting system of the robot and the delay for cleaning the exposure chamber.  
The processing is triggered with integrated data from the sample (i.e. the name of the file containing the sample data after azimuthal integration) and a list of 
buffer files corresponding to the different acquisition of buffers, usually the buffer before and the buffer after the sample acquisition.

\begin{figure}
   \label{samplechanger_worflow}
   %\includegraphics[width=12cm]{pipeline_subtract.eps}
   \includegraphics[width=12cm]{Figure_6.eps}
   \caption{Schematic of the sample-changer pipline: 
   1. Comparison of buffer curves;
   2. Subtraction of averaged buffer images from sample image;
   3. Azimuthal integration the background subtrated image;
   4. to 7. SAS analysis (contains Guinier fit, Kratky plot and BIFT analysis);
   8. Registration into ISPyB.}
\end{figure}

The sample-changer pipeline is schematized in figure \ref{samplechanger_worflow} and produces a new HDF5-file with the subtracted data in it, such file is visualized in figure \ref{subtracted} and contains the results of this 8-stage pipeline: 
\begin{enumerate}
    \item Comparison of buffer curves using the \textit{CorMap} algorithm \cite{CorMap};
    \item Buffer frames are averaged together and subtracted from the sample averaged frame (restarting from 2D raw frames);
    \item Azimuthal integration of the subtracted frame with \textit{pyFAI} \cite{pyfai_2020};
    \item Guinier analysis with the associated linear regression of $log(I(q))$ vs. $log(I_0)-\frac{R_{g}\cdot q}{3})^{2}$ at low $q$ as default plot;
    \item Dimensionless Kratky plot: $(q\cdot R_g)^2\cdot I/I_0$  vs. $q\cdot R_g$ to assess the flexibility of the macro-molecule;
    \item Porod \cite{glatter+kratky} and Rambo-Tainer invariants \cite{RamboTainerNature2013} calculation to assess the the molecular volume and molecular mass of the sample;
    \item Indirect inverse Fourier transform using the BIFT algorithm \cite{bift} provides the pair distance distribution function $p=f(r)$;
    \item Transfer of reduced data to ISPyB for BioSAXS (compatibility layer with legacy ISPyB).
\end{enumerate}
As in the \textit{multi-frame} processing pipeline (section \ref{multiframe_pipeline}), there is a link to the source data as stage zero of the processing to ensure a perfect tracking of the experiment.
The pair distribution function obtained from BIFT (stage 7) allows to calculation of the radius of gyration in real space and should confirm the radius of gyration found from Guinier fit (stage 4). 

\begin{figure}
    \label{subtracted}
    %\includegraphics[width=12cm]{subtracted.eps}
    \includegraphics[width=12cm]{Figure_7.eps}
    \caption{Default visualization of the HDF5-file produced by the sample changer pipeline with the \textit{silx} viewer. 
    The superimposed red curve corresponds to the BIFT modeled data.}
\end{figure}



%Integrated buffer data are compared with the correlation-map algorithm in order to validate that acquired buffer matches, 
%If they don't, only the buffer acquires prior to the sample acquisition is considered
%Averaging of the buffer signal and subtraction of the buffer from sample data are both performed on the 
%2D frames. 
Once again, the subtraction is made on 2D frames and not on integrated curves for similar reasons as the one presented in appendix \ref{rational}.

%Working on the 2D data frames allows a better preservation of weak signals and cancels-out systematic deviation from certain pixels.
%!! 
%this statemern needs prrof, either by citation or by showing some statistics comparing the estimated standard deviation for same data (water? BSA?) for both the old and new pipelines.
%!!
% The subtracted frame is then integrated using \textit{pyFAI} and the associated uncertainties propagated according to \cite{pyfai_2020}.

% The subtracted SAXS curve is then analyzed with Guinier fit (all three algorithms available in FreeSAS are tested)
% to provide the hydrodynamic radius of gyration, $R_g$ and the forwards scattering intensity $I_0$.
% A dimensionless Kratky plot provides some assessment on the complicity of the sample,  or if it is unfolded. 
% Finally, the inverse Fourier is obtained from BIFT and provides the diameter $D_{max}$, the pair-distribution 
% and confirms the radius of gyration.
% After the analysis, the volume is calculated from the Porod formula, and the Rambo-Tainer invariants provide  information about the molecular weight of the sample.
% All those information are registered and the default plot is defined to have a display similar to figure \ref{subtracted}
% with the scattering curve superimposed with the BIFT-fitted scattering curve.

The final stage of this pipeline is to register those results into the ISPyB for BioSAXS database \cite{ISPYBB} (https://exi.esrf.fr), and instantly available to the user via the data-portal \cite{data-portal}. 
Once data are registered into the data-portal (https://data.esrf.fr), they receive a unique DOI which can be referred to when user are publishing results to the \textsc{sasbdb} \cite{sasbdb}.
The same data are shared with the BsxCuBE3 control software via a \textit{memcached} key-value database for instant feed-back on the user interface.

\subsection{SEC-SAXS pipeline}
%I have to admit that I ma quite lost in this section. I suspect the reason is that I do not understnad how the input data looks like? Are there 10 partial chromatograms per run? 100? 1000? that makes quite a difference, doesn't it? 
Online purification of the sample allows to reduce the effect of oligomerization.
It has become a standard procedure since it was introduced to BM29 in 2012 \cite{SECPaper2012} and accounts now for two third of all measurements performed at the beam-line.

In this mode, a typical acquisition consists of one thousand frames saved as ten files containing each one hundred frames.
The input for this pipeline is a list of HDF5-files with partial chromatograms integrated by the \textit{multi-frame pipeline} presented in section \ref{multiframe_pipeline}. 
%How large is such ar partial chromatogram? How many are there? How is their size decided?
The \textsc{hplc} pipeline, figure \ref{hplc_worflow}, re-builds the complete chromatogram and performs the complete analysis of the different fractions, taking into account the possibility for empty sections (due to missing input files).
This pipeline produces files which are presented in figure \ref{hplc}.
\begin{figure}
    \label{hplc_worflow}
    %\includegraphics[width=12cm]{HPLC_pipeline.eps}
    \includegraphics[width=12cm]{Figure_8.eps}
    \caption{Schematic of the \textsc{hplc} pipline: 
    1. Concatenate ;
    2.\&3. Multivariate analysis;
    4. Background extraction;
    5. Peak finding;
    6. SAS analysis on each fraction;
    7. Storage to ISPyB.}
\end{figure}

This chromatography pipeline has seven processing stages:
\begin{enumerate}
    \item Concatenate partial chromatograms (1D curves) provided by the \textit{multi-frame} pipeline to obtain the full chromatogram; Empty/missing regions are handled here;
    % TO obtain what? All individual frames? All averages frames?
    \item Perform a Singlar Value Decomposition (SVD) on the chromatogram to assess the number of components and extract the scattering from the background \cite{BioXTAS}; 
    %on the completety, non-background corrected chromatogram?
    \item Perform a non-negative matrix factorisation (NMF) to provide the scattering curve of the different pure components and their associated chromatograms \cite{NMF-SEC_SAXS}; 
    \item Select points belonging to the background by comparing experimental scattering with the first singular-vector from the SVD; average selected curves; %If understand the following correclty, it is simply the first 30 \%? is there are reason this is not dome before SVD and NMF
    \item Perform peak-picking on subtracted curves to define fractions of the chromatogram; 
    \item Analyse each fraction with a similar pipeline to the one presented in \ref{sc-pipeline}: Guinier plot, Kratky plot, \textsc{bift} analysis ; %Where do those fractions  come from?
    \item Finally export data and send them to ISPyB for BioSAXS.
\end{enumerate}

\begin{figure}
    \label{hplc}
    \includegraphics[width=12cm]{Figure_9.eps}
    %\includegraphics[width=12cm]{HPLC-h5web.eps}
    \caption{Default visualization of the HDF5-file produced by the \textsc{hplc} pipeline with the \textit{h5web} viewer integrated into JupyterLab.}
\end{figure}

Multivariate analysis is performed to extract the signal of the macro-molecule from background scattering, and provides a hint on how many components have been separated in the chromatography.
\citeasnoun{svd_threshold} provides the number of singular values/vectors which are to be saved after the SVD decomposition
The first singular vector contains mainly background scattering, the following ones contain signal from the separated components and subsequent ones account only for noise.
Unfortunately, those singular vectors, beyond the first one, do not look like scattering curves since SVD does not enforce the positivity of those extracted singular vectors during the decomposition \cite{NNMF_spectro}. 
Unlike SVD, the NMF factorisation is neither fast nor unique, but the extracted components are all positive and look like scattering curves of components. 
% Moreover, the chromatogram for each of these components is provided.
The SVD and NMF algorithms are respectively provided by NumPy \cite{numpy} and scikit-learn \cite{sklearn}.
%This decomposes the chromatogram $I(q)$ as a serie of singular-values representing the chromatogram for every component stored in the associated singular-vector. 
%There are as many singular values/vectors pairs as time-steps acquired in the experiment but only the few firsts account for actual signal.
% The elution of the sample is usually very visible in the chromatogram associated to the second and third singular values.

%Unlike SVD, the NMF factorisation is neither fast, nor unique and the number of component needs to be known in advance.
%Five component are extracted by default, accounting for a two solvent mixture and three component to be separated.   
%As for the SVD, the first vector represents the scattering of the pure buffer and the subsequent ones 
%are representative for the different samples with the background subtracted (with the associated chromatograms).

Since none of the multivariate analysis propagates uncertainties, all processing needs to be re-done:
a correlation-map is built between the first singular vector of the SVD and all experimental scattering curves. 
Those curves are ranked from the most likely to be pure buffer to the least likely. 
Since the major part of the collected fraction are expected to be buffer, the thirty percent of the curves which are the most similar to the first singular-vector are considered to be buffer and averaged together at stage 4.
Uncertainties are propagated based on deviations calculated during azimuthal integration (and not on 2D frames, see appendix \ref{rational}).
%This should not be an issue since normalization factors, which depend mostly on transmitted intensity, vary little between frames.

%The list of curves merged for buffer is stored in the output file and exhibits gaps when the sample elutes, %providing additional hints on the sample elution time. .    
% Subsequently, all scattering curves are buffer subtracted %(here performed in 1D \ldots should we do it in 2D?) 
% and the peak-searching is .
The fraction collection (stage 5) is performed on the total scattering chromatogram, smoothed by median filtering. 
A peak search is performed with the \textit{find\_peaks\_cwt} function from the \textit{scipy} library \cite{scipy}.
It provides a list of region of high intensity scattering: the different fractions of the chromatogam.
All subtracted curves from the same fraction are averaged and analysed with a similar pipeline as described in section \ref{sc-pipeline}: Guinier fit, Kratky-plot, various invariant extraction and pair-distribution via BIFT.
The results are presented in the same way as in the sample-changer mode, one HDF5 group per fraction.

\section{Discussions}

\subsection{Statistics}
The processing described in section \ref{pipeline} is in production since September 2020 and has been operating for 20 months at the time of writing.
The table \ref{stats} summarizes some figures collected:
\begin{table}
    \label{stats}
    \caption{Statistics of the number of job run over 20 months}
    \begin{center}
    \begin{tabular}{|c|c|c|c|}
        \hline
        Processing pipeline & \#calls & Frame processed & run-time per job \\
        \hline
        Integrate multi-frame & 42575 & 1230k & 2.1 s (1 s, 10 s) \\
        Subtract \& SAS analysis & 11214 & 336k & 7.3 s \\
        \textsc{hplc} analysis & 709 & 893k & 2.9 s \\
        \hline
    \end{tabular}
    
    \end{center}
\end{table}

The run-time for \textit{multi-frame} integration presents a clear bi-modal distribution since the same code is used in sample-changer mode (10 frames/acquisition) and in \textsc{hplc} mode where 100 frames are acquired per file.
From the figures in table \ref{stats}, one can estimate \textsc{hplc}-mode represents 6\% of the measurement performed but accounts for 72\% of the total measurement time.

\subsection{Performances}
\label{performances}
Direct comparison with the former online data-analysis pipeline \cite{BM29ODA}, based on \textsc{edna}, is difficult since their structure is very different and the computer equipment has evolved.
EDNA was processing frame per frame and used to take 2.3 s to integrate a single image from the former Pilatus 1M detector.
With this new pipeline, integration occurs at more than 13 fps (table \ref{stats}) with a Pilatus 2M detector.

Performances for the \textit{sample-changer pipeline} (7.3 s) can directly be compared with two of the the former \textsc{edna} pipelines which were called \textit{autoSub} (2.1 s) and \textit{SAXSAnalysis} (9.7s).

All computations are now executed on a single computer equipped with a single hexa-core processor and an entry-level graphics card (Intel Xeon E5-2643 v3 + Nvidia Quadro M2000) which have replaced the previous setup (based on Intel Pentium4 + Quadro4000).
%From the figures shown in table \ref{stats}, the azimuthal integration for one Pilatus3 2M frame takes 0.1s (when batched by 100) which can be compared with the 2.3s which were necessary with the former EDNA-pipeline for processing one Pilatus 1M image.

%During those 20 months, the \textit{dahu}-server has been started 90 times, which corresponds to a weekly restart to apply security patches and other bug-corrections. 
%Since all data-reduction occurs within the same process using threads, this processing being performed in compiled code, any bug 
%this demonstrates not only the reliability of the \textit{dahu}-server 
%but also of the whole pipeline including \textit{FreeSAS} analysis.

\subsection{Outlook}
%Future development include adaptation for the micro-fluidic setup, and for the new ISPyB database under development. 
%Another major development will be the \textit{ab-inito} modeling based on SAXS data for which one 
%Ab-initio reconstruction
%New microfluidic setup
% new IspyB for BioSAXS

The foreseeable future should see a better integration of BioSAXS data into the data-portal with a better web visualization capabilities of the processed HDF5-files. 
%and replace of the legacy version of the ISPyB database with a new one which will include 
The buffer averaging and subtraction in \textsc{hplc} mode is not (strictly) exact since it is based on integrated curves which have been normalized.
It should be possible to weight properly those curves to obtain an average which is exactly the same as if one would have averaged or subtracted 2D frames and integrated the result (discussed in appendix \ref{rational}).
Future algorithmic work will focus on \textit{ab-initio} shape reconstruction, based on the DENSS \cite{denss} which is currently too slow to run with real-time constrains at the beam-line.

\section{Conclusion}

This document introduces the \textit{FreeSAS} and the \textit{dahu} software packages which are used respectively to analyse BioSAS data and control online data analysis.
Those two packages are used at the BioSAXS beam-line at the European synchrotron, combined with others, to provide complete data-analysis pipelines.
The three pipelines described in this contribution are used in production since 2020, and provide real-time feed-back of ongoing experiments to the user.
All metadata, all parameters and all references to the source data are recorded together with the processed data into single HDF5 files which offers 
not only convenient storage but allows also reproducible science following the \textsc{fair} principle. 

\appendix
\section{Method}

All figures were obtained from test-samples used at the beam-line: Bovine Serum Albumin (BSA) in solution at 5 mg/mL in a HEPES buffer.
The incident X-ray energy was set to 12.5 keV by a multi-layer monochromator with a band-pass of 1\% offering a flux of $1.4\times 10^{13} photons.s^{-1}$.
Experiment performed in sample-changer mode acquired ten frames of 1 s exposure each.
Experiment in \textsc{hplc} mode were using an Agilent Advance Bio SEC 300Å, 4.6 x 300 mm chromatographic column with a flow rate of 0.35 mL/min.
Acquisition was performed with 2 s exposure time and lasted 800 s (400 frames). 

\section{Rational for working with 2D frames rather than integrated curves}
\label{rational}
The \textit{multi-frame} and \textit{subtraction} pipelines are performing signal averaging and subtraction on 2D frames rather than azimuthally integrated curves.
On the other hand, the \textsc{hplc} pipeline performs the average and background subtraction on integrated curves.

This section describes the code for performing those two operations and discuses the rational behind it.
It will also try to distinguish which is the most correct. 
Let \textit{frames} and \textit{diode} be a list of acquired frames and beam-stop diode intensities, respectively.
Also assume \textit{ai} is a configured azimuthal integrator (as available from pyFAI) and \textit{npt} the number of points in the radial dimension.

\subsection{Integrate before averaging}
The code can be represent in those four lines of Python code:
\begin{verbatim} 
litg = [ai.integrate1d(frm, npt, normalization_factor=nrm, error_model="poisson") 
        for frm, nrm in zip(frames, diode)]
q = litg[0].radial
I = numpy.mean([itg.intensity for itg in litg], axis=0)
sigma = numpy.sqrt(numpy.sum([itg.sigma**2 for itg in litg], axis=0))/len(litg)
\end{verbatim}

Intensities are obtained from an arithmetic average of already weighted curves.
Uncertainties are obtained from a quadratic average of uncertainties propagated during integration. 

This method does not take into account the fact that some curves did have more weight than others during integration.
To exemplify the issue, let's consider the averaging of two buffer data sets: one with hundred frames and the second only with a single frame.
Since those buffers are all equivalent, the uncertainties for the first data set will be ten times smaller ($\sqrt{100}$) than for the second.
When merging those data sets with this method, the uncertainties of the first data set are negligible compared to the second and the propagated uncertainties will be 40\% smaller than the second data set, while in reality it should be $10\times$ smaller, ($\sqrt{101}$) ! 

As a rule of thumbs, the numerical values obtained with this method are correct when all normalisation factors are of similar magnitude.

\subsection{Sum before integrating}
The code can be represent in those three lines of Python code:
\begin{verbatim} 
img_sum = numpy.sum(frames, axis=0)
nrm_sum = sum(diode)
q, I, sigma = ai.integrate1d(img_sum, npt, normalization_factor=nrm_sum, error_model="poisson")
\end{verbatim}

The \textit{img\_sum} and \textit{nrm\_sum} are equivalent to a single exposure of the detector with much longer integration time, thus the larger normalization factor. 
This allows the contribution of different frames to be properly weighted during the azimuthal integration (see also Eq. 5 in \citeasnoun{pyfai_2020}). 

%When considering subtraction, since sample and background signal needs to be normalized before subtraction, no benefit is expected from using this approach.     

\subsection{Limitations}
In sample-changer mode, each pipeline handles only few dozens of images at a time, thus all data can easily be held in the memory of the computer.
In \textsc{hplc} mode, where several hundreds of frames are processed for a single measurement, the same technique could see the computer run out of memory.
This is why the \textsc{hplc} pipeline still averages integrated curves even if it not strictly exact, but since all normalisation factors are of same magnitude, the result is still correct.  

In pyFAI, azimuthal integration is performed with ratios of sum of signal and sum of normalization \cite{pyfai_2020}.
Those sums can be seen as partially processed data and those partial sums can be aggregated to obtain properly weighted average and uncertainties, as described in \citeasnoun{variance2018}.
Thus, the memory consumption issue could be alleviated by saving not only the averaged signal, but also the sum of signal, the sum of normalization and the partial variances and base the merging procedure on those figures rather than on the averaged value.

\ack{Acknowledgements:}
The authors wish to thank Guillaume Bonamis for his former contribution to the FreeSAS library and Jesse Hopkins from APS for the fruitful discussion and the  sharing of the code from BioXTAS-RAW.

\bibliographystyle{iucr}
\bibliography{biblio}


\end{document}                    % DO NOT DELETE THIS LINE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
