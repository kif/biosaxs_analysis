%------------------------------------------------------------------------------
% Template file for the submission of papers to IUCr journals in LaTeX2e
% using the iucr document class
% Copyright 1999-2013 International Union of Crystallography
% Version 1.6 (28 March 2013)
%------------------------------------------------------------------------------

\documentclass[preprint]{iucr}              % DO NOT DELETE THIS LINE

     %-------------------------------------------------------------------------
     % Information about journal to which submitted
     %-------------------------------------------------------------------------
     \journalcode{S}              % Indicate the journal to which submitted
                                  %   A - Acta Crystallographica Section A
                                  %   B - Acta Crystallographica Section B
                                  %   C - Acta Crystallographica Section C
                                  %   D - Acta Crystallographica Section D
                                  %   E - Acta Crystallographica Section E
                                  %   F - Acta Crystallographica Section F
                                  %   J - Journal of Applied Crystallography
                                  %   M - IUCrJ
                                  %   S - Journal of Synchrotron Radiation
\usepackage{graphicx}
\graphicspath{ {./} }

\begin{document}                  % DO NOT DELETE THIS LINE

     %-------------------------------------------------------------------------
     % The introductory (header) part of the paper
     %-------------------------------------------------------------------------

     % The title of the paper. Use \shorttitle to indicate an abbreviated title
     % for use in running heads (you will need to uncomment it).

\title{New data analysis for BioSAXS at the ESRF}
%\shorttitle{Short Title}

     % Authors' names and addresses. Use \cauthor for the main (contact) author.
     % Use \author for all other authors. Use \aff for authors' affiliations.
     % Use lower-case letters in square brackets to link authors to their
     % affiliations; if there is only one affiliation address, remove the [a].

\cauthor[a]{Jérôme}{Kieffer}{jerome.kieffer@esrf.fr}{}
\author[a]{Martha}{Brennich}
\author[b]{Jean-Baptiste}{Florial}
\author[a]{Marcus}{Oscarsson}
\author[a]{Alejandro}{De Maria Antolinos}
\author[a]{Mark}{Tully}
\author[a]{Petra}{Pernot}
\aff[a]{The European Synchrotron, 71 Avenue des Martyrs, 38000 Grenoble \country{France}}
\aff[b]{European Molecular Biology Laboratory, 71 Avenue des Martyrs, 38000 Grenoble \country{France}}


     % Use \shortauthor to indicate an abbreviated author list for use in
     % running heads (you will need to uncomment it).

\shortauthor{Kieffer and Brennich}

     % Use \vita if required to give biographical details (for authors of
     % invited review papers only). Uncomment it.

%\vita{Author's biography}

     % Keywords (required for Journal of Synchrotron Radiation only)
     % Use the \keyword macro for each word or phrase, e.g. 
     % \keyword{X-ray diffraction}\keyword{muscle}

\keyword{online data analysis; solution scattering; proteins; biological small-angle X-ray scattering; 
automation; high brilliance; structural biology;
BioSAXS; high-throughput SAXS;  size-exclusion chromatography; online purification}

     % PDB and NDB reference codes for structures referenced in the article and
     % deposited with the Protein Data Bank and Nucleic Acids Database (Acta
     % Crystallographica Section D). Repeat for each separate structure e.g
     % \PDBref[dethiobiotin synthetase]{1byi} \NDBref[d(G$_4$CGC$_4$)]{ad0002}

%\PDBref[optional name]{refcode}
%\NDBref[optional name]{refcode}

\maketitle                        % DO NOT DELETE THIS LINE

\begin{synopsis}
Detailed presentation of the automatic data analysis pipelines for the BioSAXS beam-line at the European synchrotron.  
\end{synopsis}

\begin{abstract}
The second phase of the ESRF upgrade program did not only provided a new storage ring (EBS), it also allowed to refurbish several beamlines among which the BioSAXS one which got a larger detector in a new flight tube.
To cope with the resulting increased flux of data, analysis software were rewritten to ensure real-time processing.
This article describes the tool used to interface data-analysis with beamline control (\textit{dahu}) and the various SAXS analysis algorithms needed to reduce BioSAXS data (\textit{FreeSAS}). 
Data-reduction pipelines are described for the different operation modes of the beamline either using the  sample-changer unit or using a size-exclusion chromatography setup.
\end{abstract}


     %-------------------------------------------------------------------------
     % The main body of the paper
     %-------------------------------------------------------------------------
     % Now enter the text of the document in multiple \section's, \subsection's
     % and \subsubsection's as required.

\section{Introduction}
Small angle scattering (SAS) provides low resolution information on macro-molecules and is  
particularly suited for biological samples thanks to the absence of lengthy preparation.
Unlike single crystal diffraction or NMR which offer atomic resolution, bioSAXS provides only information on the envelope of macro-molecules: it allows to validate the relative position of large structures and the assembly of biological complexes \cite{biosaxs_rev2018}. 
Biologists expects from SAXS experiment the validation of the size and the shape of their protein or complex under study. 
Since beamline users are neither synchrotron, nor SAXS specialists, a fully automated analysis is of crucial importance for them. 
Automated data analysis should provided them not only with the reduced and pre-analyzed data but also with all metadata and parameters needed to get their results published according guidelines \cite{guidelines_2017} or  reprocessed them using third party software. 

The BioSAXS beamline from the European synchortron \cite{BM29paper} had an automated pipeline for the data-analysis which was based on EDNA \cite{edna} and was using the ATSAS \cite{ATSAS2} software underneath. 
While the outcome of the processing was very appreciated by beamline users, the system was already close to the maximal throughput possible in terms of performances. 
The new EBS source \cite{EBS} of the ESRF not only provides a higher brilliance, but also new wiggler sources for the former bending-magnet beamlines which triggered the re-build and the upgrade of most of them. 
The BioSAXS beamline BM29 at the ESRF has been rebuilt in 2019 and features a new flight-tube with an upgraded  Pilatus 2M detector (planned to be) mounted in vacuum. 

This contribution is divided in two main parts, starting with a presentation of the tools used for 
processing SAS data (\textit{FreeSAS}) and for assembling pipelines (\textit{dahu}).
Then three different pipelines are presented: a common one used for the reduction of the scattering images and two dedicated for sample-changer-based experiments and another one dealing with chronological data used with the size exclusion chromatography experiments.      

\section{Tools}

To reach the 50-fold speed-up expected from the new EBS source and the new detector (upgraded from a Pilatus 1M to a 2M), all software used at the beamline has been upgraded.
Precise benchmark of the execution times of the previous EDNA-based pipeline demonstrated that 
most time was spent in launching external tools coming from the ATSAS suite and in parsing text files produced by those tools.
It was decided to rewrite all pipeline in plain Python \cite{python} and call SAS-related tasks from a library, \textit{FreeSAS}. 
Finally the interface to the control software, Bliss \cite{bliss}, would go via Tango \cite{tango} and use a simple FIFO task scheduler,  \textit{dahu}, already used in production on the TruSAXS beamline \cite{id02_2022}.   

\subsection{Small angle scattering analysis tools, \textit{FreeSAS}}

\textit{FreeSAS} is a Python library containing SAS-analysis tools available both via command line interface and from a Python API. 
It does not claim to be as complete as the ATSAS counterpart,
but is free, released under the MIT license (i.e. it can be included in commercial products), all source code is available publicly on github \cite{freesas} and
open to external contributions.
Despite Python being an interpreted language, FreeSAS is performance oriented and most of the processing is performed with Cython \cite{cython} extensions written in C to obtain the sought performances. 
All the code has been made available and packaged independently from the online analysis tools to offer ESRF users and other scientists the ability to reprocess their data and compare the results with other software.

\subsubsection{SAS plotting:} The \textit{freesas} CLI tool provide a way to plot the SAS curve: $I=f(q)$ where $q = 4\pi sin(2\theta/2)/\lambda$ together with some basic analysis which are described in the next sections.


\begin{figure}
\label{plot}
\begin{center}
\includegraphics[width=12cm]{Figure_1}
\caption{Default visualization of bioSAS data with automated analysis}
\end{center}
\end{figure}


\subsubsection{Guinier-region fitting:}
the first analysis performed on bioSAXS data is to determine the hydrodynamic radius of gyration ($R_g$) of the macromolecule and the forwards scattering value $I_0$.
$R_g$ is obtained from the slope of a linear regression of $ln(I)$ as function of $q^2$ on the proper q-range (the Guinier-region) where this curve is linear at low $q$ ($q \cdot R_g<1.3$).
The selection of the Guinier-region is far from being obvious and most of subsequent analysis depend on the $R_g$ value.
Thus multiple implementation are provided: \textit{autorg}, which derives from the implementation in BioXTAS-RAW \cite{bioxtasraw}; \textit{auto-guinier}, which searches for a consensus region rather than the best possible Guinier-region; and finally \textit{auto-gpa}, which performs a Guinier-peak-analysis \cite{gpa}. 
The later is a quick assessment of the $R_g$ and $I_0$, sometimes less robust than the two other implementations, but suitable when many datasets are to be analyzed like in chromatography mode.
It is worth mentioning that none of the three algorithms are provided the exact same results as the original \textsc{autorg} \cite{ATSAS2} version from ATSAS. 
This highlights the importance of publishing the actual implementation of the algorithms with the associated numerical parameters used in it.
  
\subsubsection{Pair distribution function:}
Despite the scattering curve $I(q)$ is the Fourier transform of the pair distribution function $p(r)$, the later cannot directly be obtained from the
inverse Fourier transform (IFT) due to the loss of phase information and the limited amount of information in the scattering curve. 
This ill-posed mathematical problem has not exact solution and is usually inverted with some extra constraints imposed, like the finite size of the support (defined by the maximum diameter, $Dmax$).    
FreeSAS proposes an IFT based on the Bayesian statistics and derived from BIFT \cite{bift}, like the one found in \cite{bioxtasraw}.
%The command-line program to perfrom and IFT is \textit{bift.py}. 
Despite different approach, the result of \textit{bift} is similar to the \textsc{datgnom} \cite{ATSAS1} from ATSAS which uses a Tikhonov's regularization.
The diameter found, $Dmax$, is directly comparable with the one provided by ATSAS, but the parameter $\alpha$ differs since the theory used for the regularisation differs. 

\subsubsection{Equivalence of scattering curves: }
To obtains the best possible signal from the sample, the capillary is exposed multiple times with a short exposure time.
All equivalent frames are then merged to optimized the signal/noise ratio without polluting the signal with radiation damage.  
The equivalence of a coupld of frames is obtained from the correlation-map algorithm implemented from the publication \citeasnoun{cormap}.

\subsubsection{Overlay of bead models:}
\textit{subpycomp} \cite{BM29ODA}, a tool to rotate/flip bead models and overlay them prior to merging them. It is equivalent to the \textsc{supcomb} \cite{supcomb} tool from ATSAS. 

\subsubsection{Extraction to ASCII data:} with the increased frame-rate of the new beamline, the historical 3-column text file with $q$, $<I>$, $\sigma(I)$ became unpractical and has been superseded by the hierarchical-data format, HDF5 \cite{hdf5} which contains all analysed data derived from raw data.
The \textit{extract-ascii} tool is designed to regenerate those text files and offer a compatibility with third-party tools.

\subsection{The job-manager: \textit{dahu}}

The role of the job manager is to ensure all the processing requested by the user interface are actually performed and that the client is warned in case of issue.
%This subsection is purely technical and can be skipped. 

EDNA was used as workflow manager for the previous data-analysis pipeline for many protein crystallography beamlines \cite{edna} and for the BioSAXS beamline at the ESRF \cite{BM29ODA}.
The parallelization model used in EDNA is based on Python threads and forking processes which was wasting resources in serializing and inter-process communication. 
%Despite the Global Interpreter Lock (GIL) which prevents Python from running multiple threads simultaneously, parallel processing occues when the work is performed in separated process. 
%This implies that for each task to be perfomed, an input file needs to be written before launching the process and the result file needs to be read and parsed after the end of the processing.
%When processing was doing loops of jobs, most of the time was spent in string manipulation for writing and parsing strings (for example: to compare pair-wise 10 frames it takes 45 process to be launched !)
 
The \textit{dahu} job-manager was designed for the TruSAXS beamline \cite{id02_2022} with those limitation in mind. 
%Text manipulation was simplified by switching from XML to JSON data-structure representation. 
%This modification alone has proven to speed up EDNA by a factor 3 !
The tango interface \cite{tango} was kept similar to the one in EDNA to ease the adoption.
%, the adaptation was  framework were kept and the code was simplified to the extreme since \textit{dahu} represents only 1000 lines of code. 
The scheduling of jobs is performed via a shared queue and only few worker are running simultaneously in threads.
Thus the code runs actually in parallel only in sections where the Global Interpreter Lock (GIL) is released, like in Cython extensions from FreeSAS or in the OpenCL code from pyFAI \cite{pyFAI_gpu}.

\subsubsection{Jobs}
%The \textit{dahu-job} 
%is the interface with the outer world. 
manage the execution identifier, exposes the status and controls the execution of the underlying \textit{plugins}. %via the \textit{setup-process-teardown} sequence.
%Unlike EDNA, in \textit{dahu}, only 
Jobs see their input and output saved to the disk, this allow off-line reprocessing in case of issue during online data analysis.

\subsubsection{Plugins}
are Python \textit{functions} or \textit{classes} which are provided externally (by the beamline scientist) and implement the actual processing pipeline.
Those plugins are fairly independent from the \textit{dahu} framework and easy to implement in pure Python.
%For instance example the three plugins described in section \ref{pipeline} account each for about 700 lines of code.  

%\subsubsection{Online processing}
%Online processing is achieved by exposing \textit{dahu-jobs} via a Tango interface.

\subsubsection{Offline re-processing}
is possible from the command-line tool \textit{dahu-reprocess}.
This tool was designed to (re-)execute one or several jobs based on the description saved by the online data analysis server. 
% This tool is also used for testing analysis pipelines offline.
Dahu has virtually no dependencies (beside \textit{numpy} and \textit{tango} for the server) and can be deployed on any computer to reprocess complete datasets. 
Neverthless, to reprocess data acquired at the BioSAXS beamline, one would of course need FreeSAS and pyFAI and all the other dependencies, but all of them are publicly available and open source.

\section{Data analysis pipelines}

There are two main experiments performed at the BioSAXS beamline, either using the sample changer or the inline-chromatography setup.
Thus, two analysis pipelines were built, one for each of those experimental setups, and a third pipeline acting as a pre-processor and containing the common part related to the azimuthal integration of individual frames.

Like all new detectors acquired at the ESRF during the second phase of the upgrade program, the Pilatus 2M detector used on the BioSAXS beamline is controled by the LIMA \cite{lima} software which saves images in a HDF5 file-format \cite{hdf5} with several (~100) frames per file.
This permits compression, faster data-access, symbolic links to datasets from one file to another ... but it is dramatically different from the former pipeline which was triggered frame per frame.

The versatility of the HDF5 format allows to have a single output file for each of the processing pipeline, making backups easier as well. 
Each pipeline registers the result of every processing step of the pipeline in the HDF5 file together with the associated configuration.
Input datasets are also referenced using external links to the raw data to avoid data duplication. 
Finally metadata describing the sample, its buffer and the configuration of the beamline are also registered using the Nexus convention \cite{nexus}.
Each processing pipeline produces a single HDF5 file as output, which defines a \textit{default} plot which tries to best summarize the experimental result to the user.
ESRF provides several tools to visualize those HDF5 files like \textit{silx view} \cite{silx} which is a graphical user interface based on Qt \cite{pyqt} or the web-viewer \textit{h5web} \cite{h5web} to visualize those files inside a web-browser.
This \textit{h5web} is already used in the beamline control user interface BsxCuBE3 \cite{bm29_2022} and will be used in the new ISPyB interface (under development).

\subsection{Multi-frame integration pipeline}
\label{multiframe_pipeline}
The figure \ref{multiframe} presents a file produced by the \textit{multi-frame} integration pipeline with the default visualization consisting in the curve $I = f(q)$. 
There are four subsequent analysis steps:
\begin{enumerate}
\item Each frame is azimuthally integrated to produce scattering curves
\item Scattering curves are compared to to assess radiation damage
\item Equivalent frames are averaged together
\item The averaged frame is finally azimuthally integrated
\end{enumerate}
The HDF5-file contains in addition external links to the raw frames as acquired by the detector.

\begin{figure}
\caption{Layout of a HDF5-file obtained from the multi-frame integration pipeline, visualized with the \textit{silx} software \cite{silx}.}
\includegraphics{multiframe}
\label{multiframe}
\end{figure}

Azimuthal integration is a common reduction step for all small angle scattering experiments. 
The BioSAXS beamline uses \textit{pyFAI} \cite{pyfai_2020} as core of its \textit{multiframe} integration pipeline.
Thanks to the GPU-computing, \textit{pyFAI} is able to integrate each frame within a millisecond.
In order to avoid cross-correlation between neighboring bins, pixel splitting is disabled.
Frames are read from the HDF5 file produced by \textit{LImA} \cite{lima} and all other metadata like the sample description and the transmitted  intensity (recorded on the beam-stop diode) are sent as part of the job description from BsxCuBE3, the graphical user interface running the beamline.

In sample changer mode, all frames are expected to be similar and needs to be merged, except the ones exhibiting radiation damage which should be discarded.
For this, every single scattering curve is compared with all others to build the correlation-map \cite{cormap}. 
This map allows to determine the largest group of frames which are equivalent based on two threshold, the minimum probability for two adjacent frames to be the same and the threshold for frames which are not adjacent. 

Frames found equivalent in the correlation-map analysis are averaged taking into account normalization from the beam-stop diode value. 
The uncertainties for pixel values is propagated assuming Poisson statistics. 
%The standard deviation of each pixel in the stack of equivalent frames was found noticably lower than the propagated variance, assuming a Poisson statistic.
The averaged frame (with its associated uncertainties) is finally azimuthally integrated with pyFAI and stored in the output file. 
The SAXS plot $I = f(q)$ is set as the default representation of this output file.

In chromatography mode, the acquisition occurs typically over half an hour representing thousand images at 1Hz acquisition rate. 
This results in several HDF5-files containing partial chromatograms.
The total scattering is computed by summing all intensities in each of the curve to build a simple chromatogram $I_{sum} = f(t)$ 
which indicates weather a sample eluted from the chromatography column or not. 
This (partial) chromatogram is the default plot for this type of integrated file.

\subsection{Sample-changer pipeline}
In sample changer mode, solution containing samples are acquired interleaved with pure buffer solutions.
The processing is triggered with the sample filename, i.e. the name of the file containing the sample data after azimuthal integration and a list of 
buffer files corresponding to the acquisition of buffers (usually the buffer before and the buffer after the sample acquisition). 
This produces a new HDF5 file with the subtracted data in it, such file is visualized in figure \ref{subtracted}
and contains the results of the 7-stage pipeline: 
\begin{enumerate}
    \item Comparison of buffer curves;
    \item Buffer subtraction on the frame level;
    \item Azimuthal integration of the subtracted frame;
    \item Guinier analysis;
    \item Dimensionless Kratky plot;
    \item Rambo-Tainer invariant calculation;
    \item Fourier inversion using the BIFT algorithm.
\end{enumerate}
As previously, there is a link to the source data as stage zero of the processing to ensure a perfect tracking of the experiment.

\begin{figure}
\label{subtracted}
\begin{center}
\includegraphics{Figure_1}
\caption{Default visualization of the HDF5-file produced by the sample changer pipeline with the \textit{silx} viewer}
\end{center}
\end{figure}


Integrated buffer data are compared with the correlation-map algorithm in order to validate that acquired buffer matches, 
If they don't, only the buffer acquires prior to the sample acquisition is considered
Averaging of the buffer signal and subtraction of the buffer from sample data are both performed on the 
2D frames. 
Working on the 2D data frames allows a better preservation of weak signals and cancels-out systematic deviation
from certain pixels.
The subtracted frame is then integrated using \textit{pyFAI} and the associated uncertainties propagated according to \cite{pyfai_2020}.

The subtracted SAXS curve is then analyzed with Guinier fit (all three algorithms available in FreeSAS are tested)
to provide the hydrodynamic radius of gyration, $R_g$ and the forwards scattering intensity $I_0$.
A dimensionless Kratky plot provides some assessment on the complicity of the sample,  or if it is unfolded. 
Finally, the inverse Fourier is obtained from BIFT and provides the diameter $D_{max}$, the pair-distribution 
and confirms the radius of gyration.
After the analysis, the volume is calculated from the Porod formula, and the Rambo-Tainer invariants provide 
information about the molecular weight of the sample.
All those information are registered and the default plot is defined to have a display similar to figure \ref{subtracted}
with the scattering curve superimposed with the BIFT-fitted scattering curve.

The final stage of this pipeline is to register those results into the ISPyB for BioSAXS \cite{ISPYBB} database and instantly made available to the user via the BioSAXS data portal (https://exi2.esrf.fr). 
The same data are shared with the BSXCube control software via a memcached key-value database for instant feed-back.

\subsection{SEC-SAXS pipeline}
Online purification of the sample to reduce the effect of oligomerization has become a \textit{de-facto} procedure
since it was introduced in 2012 \cite{SECPaper2012} and accounts for half of the experiments performed at the beamline.

The input for this pipeline a list of files with partial chromatograms integrated by the multi-frame pipeline presented in section \ref{multiframe_pipeline}.
The first step is to re-build the complete chromatogram, taking into account the possibility for empty sections (due to missing input files), and is presented in figures \ref{hplc}.
This chromatography pipeline has six stages:
\begin{enumerate}
    \item Rebuild the chromatogram from partial ones provided by the \textsc{multi-frame} pipeline;
    \item Perform the Singlar Value Decomposition to assess the number of components;
    \item Perform the NMF decomposition to get an;
    \item Average out the background curves;
    \item Analyse each fraction with a similar pipeline to the one presented in \ref{subtracted}
    \item Prepare the data and send them to ISPyB for BioSAXS.
\end{enumerate}

\begin{figure}
\label{hplc}
\begin{center}
\includegraphics{hplc}
\caption{Default visualization of the HDF5-file produced by the \textsc{hplc} pipeline with the \textit{silx} viewer}
\end{center}
\end{figure}


Multivariate analysis is performed to extract the signal of the macromolecule from background scattering, and provide a hint on how many components have been separated in the chromatogram. 
First the singular value decomposition (SVD) is performed, provided by the NumPy package \cite{numpy}.
This decomposes the chromatogram $I(q)$ as a serie of singular-values representing the chromatogram for every component stored in the associated singular-vector. 
There are as many singular values/vectors pairs as time-steps acquired in the experiment but only the first ones account for actual signal.
Subsequent ones account only for noise. 
The number of saved singular saved is provided in \cite{svd_threshold}.
The elution of the sample is usually very visible in the chromatogram associated to the second and third singular values.
The first singular vector contains mainly background scattering
Users are sometimes surprised by the extracted singular vectors which should represent the scattering of the pure sample but contain some negative section.
Indeed, SVD does not enforce the signal to be positive and is a pure mathematical transformation.

Another factorisation of the data is performed with non-negative matrix factorisation (NMF, provided by the scikit-learn library \cite{sklearn}) since it enforces the positivity of the scattering contribution. 
Unlike SVD, the NMF factorisation is neither fast, nor unique and the number of component needs to be known in advance.
%Five component are extracted by default, accounting for a two solvent mixture and three component to be separated.   
As for the SVD, the first vector represents the scattering of the pure buffer and the subsequent ones 
are representative for the different samples with the background subtracted (with the associated chromatograms).

Since none of the multivariate analysis propagates uncertainties, the scattering curve of the pure buffer is obtained from the first singular vector of the SVD.
A \textit{correlation-map} is built with all experimental scattering curves and ranked from the  most likely to be pure buffer to the least likely. 
Since the major part of the collected fraction are constituted of buffer, the first thirty percent are considered to be pure buffer and averaged together.
%The list of curves merged for buffer is stored in the output file and exhibits gaps when the sample elutes, 
%providing additional hints on the sample elution time. .    
Subsequently, all scattering curves are buffer subtracted %(here performed in 1D \ldots should we do it in 2D?) 
and the peak-searching is performed on the total scattering chromatogram, after median filtering for smoothing.
This peak search is performed with the \textit{find\_peaks\_cwt} function from the \textit{scipy} library \cite{scipy} which provides a list of peaks which are subsequently analyzed.

Each fraction should correspond to a unique component, thus all subtracted curves associated to this region are merged and analysed with a similar pipeline as with the sample-changer mode: Guinier analysis, Kratky-plot, various invariant extraction and pair-distribution via BIFT. 
The results are presented the same way as in the sample-changer mode, one per fraction.
The criteria for the fraction selection being intentionally soft, it is common that empty fractions are selected and that some or even all analysis fail on them. 

%\section{Discussions}
%Ab-initio reconstruction
%New microfluidic setup
% new IspyB for BioSAXS

\section{Conclusion}

This paper introduces the \textit{FreeSAS} and the \textit{dahu} software packages which are used respectively to analyse bio-SAS data and control online data analysis. 
Those two packages are combined with other to provide complete data analysis pipelines.
The three pipelines described in this contribution are used in production at the BioSAXS beamline 
since 2020, and they provide real-time feed-back of ongoing experiment to the user.
All metadata, all parameters and all references to the data source are recorded together with the processed data into
a single HDF5 files which offers convenient storage and allows reproducible science in the FAIR principle. 

The foreseeable future should replace of the legacy version of the ISPyB database with a new one which will include better web visualization capabilities of those HDF5 files.
Future algorithmic work will focus on \textit{ab-initio} shape reconstruction which is currently too slow to run with real-time constrains at the beamline.


\ack{Acknowledgements:}
The authors wish to thank Guillaume Bonamis for his former contribution to the FreeSAS library and Jesse Hopkins from APS for the fruitful discussion and the the sharing of the code from BioXTAS-RAW.

%\begin{references}
%\reference{Author, A. \& Author, B. (1984). \emph{Journal} \textbf{Vol}, 
%first page--last page.}
%\end{references}


\bibliographystyle{iucr}
\bibliography{biblio}


\end{document}                    % DO NOT DELETE THIS LINE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
